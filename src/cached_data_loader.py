"""
SKEP DataAnalysis - Cached Data Loader
======================================

ì‚¬ì „ ì²˜ë¦¬ëœ ìºì‹œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” í´ë˜ìŠ¤
Dashboard Modeì—ì„œ ë¹ ë¥¸ ë°ì´í„° ì ‘ê·¼ì„ ìœ„í•´ ì‚¬ìš©
"""

import json
from pathlib import Path
from typing import Dict, List, Optional, Any

import pandas as pd


class CachedDataLoader:
    """ìºì‹œëœ ë¶„ì„ ë°ì´í„° ë¡œë”"""
    
    def __init__(self, cache_folder: str):
        self.cache_folder = Path(cache_folder)
        self._cache: Dict[str, Any] = {}
        self._metadata: Optional[Dict] = None
    
    def is_valid(self) -> bool:
        """ìºì‹œê°€ ìœ íš¨í•œì§€ í™•ì¸"""
        metadata_path = self.cache_folder / "metadata.json"
        return metadata_path.exists()
    
    def get_metadata(self) -> Dict:
        """ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        if self._metadata is None:
            metadata_path = self.cache_folder / "metadata.json"
            if metadata_path.exists():
                with open(metadata_path, 'r') as f:
                    self._metadata = json.load(f)
            else:
                self._metadata = {}
        return self._metadata
    
    def _load_parquet(self, filename: str) -> pd.DataFrame:
        """Parquet íŒŒì¼ ë¡œë“œ (ìºì‹±)"""
        if filename not in self._cache:
            path = self.cache_folder / filename
            if path.exists():
                self._cache[filename] = pd.read_parquet(path)
            else:
                self._cache[filename] = pd.DataFrame()
        return self._cache[filename]
    
    def _load_json(self, filename: str) -> Any:
        """JSON íŒŒì¼ ë¡œë“œ (ìºì‹±)"""
        if filename not in self._cache:
            path = self.cache_folder / filename
            if path.exists():
                with open(path, 'r') as f:
                    self._cache[filename] = json.load(f)
            else:
                self._cache[filename] = {}
        return self._cache[filename]
    
    # ========== T31 (ì¥ë¹„) ë°ì´í„° ==========
    
    def load_t31_hourly_activity(self) -> pd.DataFrame:
        """T31 ì‹œê°„ëŒ€ë³„ í™œë™"""
        return self._load_parquet("t31_results_hourly_activity.parquet")
    
    def load_t31_device_stats(self) -> pd.DataFrame:
        """T31 ì¥ë¹„ë³„ í†µê³„"""
        return self._load_parquet("t31_results_device_stats.parquet")
    
    def load_t31_sward_activity(self) -> pd.DataFrame:
        """T31 S-Wardë³„ í™œë™"""
        return self._load_parquet("t31_results_sward_activity.parquet")
    
    def load_t31_two_min_unique(self) -> pd.DataFrame:
        """T31 2ë¶„ ë‹¨ìœ„ unique MAC ì¹´ìš´íŠ¸"""
        return self._load_parquet("t31_results_two_min_unique_mac.parquet")
    
    def load_t31_operation_heatmap(self) -> pd.DataFrame:
        """T31 Operation Heatmap ë°ì´í„° (10ë¶„ ë‹¨ìœ„)"""
        return self._load_parquet("t31_results_operation_heatmap.parquet")
    
    # ========== T41 (ì‘ì—…ì) ë°ì´í„° ==========
    
    def load_t41_occupancy(self) -> pd.DataFrame:
        """T41 ì‹œê°„ëŒ€ë³„ ì‘ì—…ì ìˆ˜"""
        return self._load_parquet("t41_results_occupancy.parquet")
    
    def load_t41_worker_dwell(self) -> pd.DataFrame:
        """T41 ì‘ì—…ìë³„ ì²´ë¥˜ì‹œê°„"""
        return self._load_parquet("t41_results_worker_dwell.parquet")
    
    def load_t41_building_occupancy(self) -> pd.DataFrame:
        """T41 Building/Levelë³„ ì‘ì—…ì ìˆ˜"""
        return self._load_parquet("t41_results_building_occupancy.parquet")
    
    def load_t41_space_type_stats(self) -> pd.DataFrame:
        """T41 ê³µê°„ ìœ í˜•ë³„ í†µê³„"""
        return self._load_parquet("t41_results_space_type_stats.parquet")
    
    def load_t41_journey_data(self) -> pd.DataFrame:
        """T41 ì‘ì—…ì ì´ë™ ê²½ë¡œ"""
        return self._load_parquet("t41_results_journey_data.parquet")
    
    def load_t41_activity_analysis(self) -> pd.DataFrame:
        """T41 1ë¶„ ë‹¨ìœ„ í™œë™ ë¶„ì„ (Journey Heatmapìš©)"""
        return self._load_parquet("t41_results_activity_analysis.parquet")
    
    def load_t41_journey_heatmap(self) -> pd.DataFrame:
        """T41 Journey Heatmap ì „ìš© precomputed ë°ì´í„° (10ë¶„ ë‹¨ìœ„)
        
        Returns:
            DataFrame with columns: mac, bin_index, building_level, building, level, signal_count, color_code
        """
        return self._load_parquet("t41_results_journey_heatmap.parquet")
    
    def load_t41_two_min_unique(self) -> pd.DataFrame:
        """T41 2ë¶„ ë‹¨ìœ„ unique MAC ì¹´ìš´íŠ¸"""
        return self._load_parquet("t41_results_two_min_unique_mac.parquet")
    
    def load_t41_hourly_avg_from_2min(self) -> pd.DataFrame:
        """T41 ì‹œê°„ëŒ€ë³„ í‰ê·  (2ë¶„ ë‹¨ìœ„ ê¸°ë°˜)"""
        return self._load_parquet("t41_results_hourly_avg_from_2min.parquet")
    
    def get_t41_worker_counts(self) -> Dict[str, int]:
        """T41 ì‘ì—…ì ìˆ˜ (í•„í„°ë§ ì „/í›„)"""
        total = self._load_json("t41_results_total_worker_count.json")
        filtered = self._load_json("t41_results_filtered_worker_count.json")
        return {
            'total': total if isinstance(total, int) else 0,
            'filtered': filtered if isinstance(filtered, int) else 0
        }
    
    # ========== Flow (ìŠ¤ë§ˆíŠ¸í°) ë°ì´í„° ==========
    
    def load_flow_hourly(self) -> pd.DataFrame:
        """Flow ì‹œê°„ëŒ€ë³„ ìœ ë™ì¸êµ¬"""
        return self._load_parquet("flow_results_hourly_flow.parquet")
    
    def load_flow_device_stats(self) -> pd.DataFrame:
        """Flow ë””ë°”ì´ìŠ¤ë³„ í†µê³„"""
        return self._load_parquet("flow_results_device_stats.parquet")
    
    def load_flow_sward(self) -> pd.DataFrame:
        """Flow S-Wardë³„ ìœ ë™ì¸êµ¬"""
        return self._load_parquet("flow_results_sward_flow.parquet")
    
    def load_flow_two_min_unique(self) -> pd.DataFrame:
        """Flow 2ë¶„ ë‹¨ìœ„ unique MAC ì¹´ìš´íŠ¸ (í•µì‹¬)"""
        return self._load_parquet("flow_results_two_min_unique_mac.parquet")
    
    def load_flow_hourly_avg_from_2min(self) -> pd.DataFrame:
        """Flow ì‹œê°„ëŒ€ë³„ í‰ê·  (2ë¶„ ë‹¨ìœ„ ê¸°ë°˜)"""
        return self._load_parquet("flow_results_hourly_avg_from_2min.parquet")
    
    def load_flow_ten_min_unique(self) -> pd.DataFrame:
        """Flow 10ë¶„ ë‹¨ìœ„ unique MAC ì¹´ìš´íŠ¸"""
        return self._load_parquet("flow_results_ten_min_unique.parquet")
    
    def load_flow_device_type_stats(self) -> pd.DataFrame:
        """Flow ë””ë°”ì´ìŠ¤ íƒ€ì…ë³„ í†µê³„ (Apple/Android)"""
        return self._load_parquet("flow_results_device_type_stats.parquet")
    
    # ========== í†µí•© ë°ì´í„° ==========
    
    def get_summary(self) -> Dict:
        """ì „ì²´ ë°ì´í„° ìš”ì•½"""
        return self._load_json("combined_results_summary.json")
    
    # ========== Dashboard Overview ë°ì´í„° ==========
    
    def load_t31_building_level_equipment(self) -> pd.DataFrame:
        """T31 ê±´ë¬¼/ì¸µë³„ ì¥ë¹„ ìˆ˜ (Primary Location ê¸°ì¤€)"""
        return self._load_parquet("dashboard_results_t31_building_level_equipment.parquet")
    
    def load_t31_mac_primary_location(self) -> pd.DataFrame:
        """T31 MACë³„ Primary Location ì •ë³´"""
        return self._load_parquet("dashboard_results_t31_mac_primary_location.parquet")
    
    def load_t31_ten_min_operation_rate(self) -> pd.DataFrame:
        """T31 10ë¶„ ë‹¨ìœ„ ê°€ë™ë¥ """
        return self._load_parquet("dashboard_results_t31_ten_min_operation_rate.parquet")
    
    def load_t31_hourly_operation_rate(self) -> pd.DataFrame:
        """T31 ì‹œê°„ëŒ€ë³„ ê°€ë™ë¥ """
        return self._load_parquet("dashboard_results_t31_hourly_operation_rate.parquet")
    
    def load_t31_building_hourly_active(self) -> pd.DataFrame:
        """T31 ê±´ë¬¼ë³„ ì‹œê°„ëŒ€ë³„ í™œì„± ì¥ë¹„"""
        return self._load_parquet("dashboard_results_t31_building_hourly_active.parquet")
    
    def load_t31_equipment_positions(self) -> pd.DataFrame:
        """T31 ì¥ë¹„ ìœ„ì¹˜ ì¢Œí‘œ"""
        return self._load_parquet("dashboard_results_t31_equipment_positions.parquet")
    
    def load_t41_building_level_workers(self) -> pd.DataFrame:
        """T41 ê±´ë¬¼/ì¸µë³„ ì‘ì—…ì ìˆ˜"""
        return self._load_parquet("dashboard_results_t41_building_level_workers.parquet")
    
    def load_t41_hourly_workers(self) -> pd.DataFrame:
        """T41 ì‹œê°„ëŒ€ë³„ ì‘ì—…ì ìˆ˜"""
        return self._load_parquet("dashboard_results_t41_hourly_workers.parquet")
    
    def load_t41_building_hourly_workers(self) -> pd.DataFrame:
        """T41 ê±´ë¬¼ë³„ ì‹œê°„ëŒ€ë³„ ì‘ì—…ì ìˆ˜"""
        return self._load_parquet("dashboard_results_t41_building_hourly_workers.parquet")
    
    def load_t41_building_level_hourly_workers(self) -> pd.DataFrame:
        """T41 ê±´ë¬¼-ì¸µë³„ ì‹œê°„ëŒ€ë³„ ì‘ì—…ì ìˆ˜"""
        return self._load_parquet("dashboard_results_t41_building_level_hourly_workers.parquet")
    
    def get_t41_busiest_location(self) -> Dict:
        """T41 ê°€ì¥ í˜¼ì¡í•œ ìœ„ì¹˜"""
        return self._load_json("dashboard_results_t41_busiest_location.json")
    
    def load_t41_ten_min_workers(self) -> pd.DataFrame:
        """T41 10ë¶„ ë‹¨ìœ„ ì‘ì—…ì ìˆ˜"""
        return self._load_parquet("dashboard_results_t41_ten_min_workers.parquet")
    
    def load_t41_building_ten_min_workers(self) -> pd.DataFrame:
        """T41 ê±´ë¬¼ë³„ 10ë¶„ ë‹¨ìœ„ ì‘ì—…ì ìˆ˜"""
        return self._load_parquet("dashboard_results_t41_building_ten_min_workers.parquet")
    
    # ========== T41 Active/Inactive Stats (10ë¶„ ë‹¨ìœ„, Building/Levelë³„) ==========
    
    def load_t41_stats_10min(self, building: str = "All", level: str = "All") -> pd.DataFrame:
        """T41 10ë¶„ ë‹¨ìœ„ Active/Inactive Stats ë¡œë“œ
        
        Args:
            building: "All" ë˜ëŠ” íŠ¹ì • ë¹Œë”©ëª… (ì˜ˆ: "WWT", "FAB")
            level: "All" ë˜ëŠ” íŠ¹ì • ì¸µ (ì˜ˆ: "1F", "B1F")
            
        Returns:
            DataFrame with columns: [bin_index, Total, Active, Inactive, time_label, filter]
        """
        if building == "All":
            return self._load_parquet("dashboard_results_t41_stats_10min_all.parquet")
        elif level == "All":
            return self._load_parquet(f"dashboard_results_t41_stats_10min_{building}.parquet")
        else:
            return self._load_parquet(f"dashboard_results_t41_stats_10min_{building}_{level}.parquet")
    
    def get_available_t41_stats_filters(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ T41 Stats í•„í„° ëª©ë¡"""
        filters = ["All"]
        for f in self.cache_folder.glob("dashboard_results_t41_stats_10min_*.parquet"):
            name = f.stem.replace("dashboard_results_t41_stats_10min_", "")
            if name != "all":
                filters.append(name.replace("_", "-"))
        return sorted(filters)

    # ========== T-Ward vs Mobile ë¹„êµ ë°ì´í„° ==========
    
    def load_tvm_comparison(self, building: str = "All", level: str = "All") -> pd.DataFrame:
        """T-Ward vs Mobile ë¹„êµ ë°ì´í„° ë¡œë“œ
        
        Args:
            building: "All" ë˜ëŠ” íŠ¹ì • ë¹Œë”©ëª…
            level: "All" ë˜ëŠ” íŠ¹ì • ì¸µ
            
        Returns:
            DataFrame with columns: [bin_index, t41_count, mobile_count, time_label, ratio, filter]
        """
        if building == "All":
            return self._load_parquet("dashboard_results_tvm_comparison_all.parquet")
        elif level == "All":
            return self._load_parquet(f"dashboard_results_tvm_comparison_{building}.parquet")
        else:
            return self._load_parquet(f"dashboard_results_tvm_comparison_{building}_{level}.parquet")

    # ========== Journey Heatmap ì‚¬ì „ ê³„ì‚° ë°ì´í„° ==========
    
    def load_journey_heatmap_sorted(self, sort_option: str = "ai", max_workers: int = 200) -> pd.DataFrame:
        """ì •ë ¬ëœ Journey Heatmap ë°ì´í„° ë¡œë“œ
        
        Args:
            sort_option: "ai", "dwell", "building", "signal"
            max_workers: 50, 100, 150, 200, 250, 300, 350, 400, 450, 500
            
        Returns:
            DataFrame with worker order and color codes
        """
        filename = f"dashboard_results_journey_heatmap_{sort_option}_{max_workers}.parquet"
        return self._load_parquet(filename)
    
    def get_available_journey_options(self) -> Dict:
        """ì‚¬ìš© ê°€ëŠ¥í•œ Journey Heatmap ì˜µì…˜"""
        sort_options = []
        max_workers_options = []
        
        for f in self.cache_folder.glob("dashboard_results_journey_heatmap_*.parquet"):
            name = f.stem.replace("dashboard_results_journey_heatmap_", "")
            parts = name.rsplit("_", 1)
            if len(parts) == 2:
                sort_options.append(parts[0])
                max_workers_options.append(int(parts[1]))
        
        return {
            'sort_options': list(set(sort_options)),
            'max_workers': sorted(list(set(max_workers_options)))
        }

    def load_flow_hourly_devices(self) -> pd.DataFrame:
        """Flow ì‹œê°„ëŒ€ë³„ ìœ ë™ì¸êµ¬"""
        return self._load_parquet("dashboard_results_flow_hourly_devices.parquet")
    
    # ========== AI Insights ==========
    
    def load_ai_insights(self, data_type: str = 't41') -> Optional[Dict]:
        """AI ì¸ì‚¬ì´íŠ¸ ë¡œë“œ (í†µí•© ì¸í„°í˜ì´ìŠ¤)
        
        Args:
            data_type: 't31', 't41', 'flow', 'combined'
        """
        if data_type == 't31':
            return self._load_json("ai_insights_t31_overview.json")
        elif data_type == 't41':
            return self._load_json("ai_insights_t41_overview.json")
        elif data_type == 'flow':
            return self._load_json("ai_insights_flow_overview.json")
        elif data_type == 'combined':
            return self._load_json("ai_insights_combined_overview.json")
        return None
    
    def get_t41_congestion_info(self) -> Optional[Dict]:
        """T41 í˜¼ì¡ë„ ì •ë³´"""
        insights = self._load_json("ai_insights_t41_overview.json")
        # insightsê°€ Dictì´ê³  congestion_scoreê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬
        if isinstance(insights, dict) and 'congestion_score' in insights:
            return {
                'score': insights.get('congestion_score', 0),
                'level': insights.get('congestion_level', 'Unknown'),
                'peak_time': insights.get('peak_time', 'N/A'),
                'peak_workers': insights.get('peak_workers', 0)
            }
        return None
    
    def get_t31_ai_insight(self) -> str:
        """T31 AI ì¸ì‚¬ì´íŠ¸ í…ìŠ¤íŠ¸"""
        return self._load_json("ai_insights_t31_overview.json")
    
    def get_t31_ai_summary(self) -> Dict:
        """T31 AI ìš”ì•½ ë°ì´í„°"""
        return self._load_json("ai_insights_t31_summary.json")
    
    def get_t41_ai_insight(self) -> str:
        """T41 AI ì¸ì‚¬ì´íŠ¸ í…ìŠ¤íŠ¸"""
        return self._load_json("ai_insights_t41_overview.json")
    
    def get_t41_ai_summary(self) -> Dict:
        """T41 AI ìš”ì•½ ë°ì´í„°"""
        return self._load_json("ai_insights_t41_summary.json")
    
    def get_flow_ai_insight(self) -> str:
        """Flow AI ì¸ì‚¬ì´íŠ¸ í…ìŠ¤íŠ¸"""
        return self._load_json("ai_insights_flow_overview.json")
    
    def get_combined_ai_insight(self) -> str:
        """í†µí•© AI ì¸ì‚¬ì´íŠ¸"""
        return self._load_json("ai_insights_combined_overview.json")
    
    # ========== íˆíŠ¸ë§µ ë°ì´í„° ==========
    
    def load_heatmap(self, building: str, level: str) -> pd.DataFrame:
        """íŠ¹ì • Building/Levelì˜ íˆíŠ¸ë§µ ë°ì´í„°"""
        filename = f"heatmap_results_heatmap_t41_{building}_{level}.parquet"
        return self._load_parquet(filename)
    
    def get_available_heatmaps(self) -> List[Dict[str, str]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ íˆíŠ¸ë§µ ëª©ë¡"""
        heatmaps = []
        for f in self.cache_folder.glob("heatmap_results_heatmap_t41_*.parquet"):
            # íŒŒì¼ëª…ì—ì„œ building, level ì¶”ì¶œ
            name = f.stem.replace("heatmap_results_heatmap_t41_", "")
            parts = name.rsplit("_", 1)
            if len(parts) == 2:
                heatmaps.append({
                    'building': parts[0],
                    'level': parts[1],
                    'filename': f.name
                })
        return heatmaps
    
    # ========== ì›ë³¸ ë°ì´í„° ë¡œë“œ (ê¸°ì¡´ ë¶„ì„ ê¸°ëŠ¥ ì‚¬ìš©ì„ ìœ„í•´) ==========
    
    def load_raw_t31(self) -> pd.DataFrame:
        """ì›ë³¸ T31 ë°ì´í„° ë¡œë“œ"""
        return self._load_parquet("raw_t31.parquet")
    
    def load_raw_t41(self) -> pd.DataFrame:
        """ì›ë³¸ T41 ë°ì´í„° ë¡œë“œ"""
        return self._load_parquet("raw_t41.parquet")
    
    def load_raw_flow(self) -> pd.DataFrame:
        """ì›ë³¸ Flow ë°ì´í„° ë¡œë“œ"""
        return self._load_parquet("raw_flow.parquet")
    
    def load_raw_sward_config(self) -> pd.DataFrame:
        """ì›ë³¸ S-Ward ì„¤ì • ë¡œë“œ"""
        return self._load_parquet("raw_sward_config.parquet")
    
    def has_raw_data(self) -> Dict[str, bool]:
        """ì›ë³¸ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        return {
            't31': (self.cache_folder / "raw_t31.parquet").exists(),
            't41': (self.cache_folder / "raw_t41.parquet").exists(),
            'flow': (self.cache_folder / "raw_flow.parquet").exists(),
            'sward_config': (self.cache_folder / "raw_sward_config.parquet").exists(),
        }
    
    def clear_cache(self):
        """ë©”ëª¨ë¦¬ ìºì‹œ ì´ˆê¸°í™”"""
        self._cache.clear()
        self._metadata = None


def find_available_datasets(base_folder: str = None) -> List[Dict]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ (ìºì‹œ ìˆëŠ”) ëª©ë¡"""
    import os
    import streamlit as st
    datasets = []
    
    # ì—¬ëŸ¬ ê²½ë¡œ í›„ë³´ ì‹œë„ (Streamlit Cloud í˜¸í™˜)
    if base_folder is None:
        path_candidates = []
        
        # 1. í˜„ì¬ íŒŒì¼ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ
        current_file = Path(__file__).resolve()
        path_candidates.append(current_file.parent.parent / "Datafile" / "Rawdata")
        
        # 2. í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€
        cwd = Path(os.getcwd())
        path_candidates.append(cwd / "Datafile" / "Rawdata")
        
        # 3. Streamlit Cloud í™˜ê²½ - ì—¬ëŸ¬ ì¼€ì´ìŠ¤ ì‹œë„
        path_candidates.append(Path("/mount/src/irfm_demo/Datafile/Rawdata"))
        path_candidates.append(Path("/mount/src/IRFM_demo/Datafile/Rawdata"))
        path_candidates.append(Path("/mount/src/irfm-demo/Datafile/Rawdata"))
        path_candidates.append(Path("/app/Datafile/Rawdata"))
        
        # ë””ë²„ê·¸: ì‚¬ì´ë“œë°”ì— ê²½ë¡œ ì •ë³´ í‘œì‹œ
        debug_info = []
        for i, candidate in enumerate(path_candidates):
            exists = candidate.exists()
            debug_info.append(f"{i+1}. {candidate}: {'âœ…' if exists else 'âŒ'}")
        
        with st.sidebar.expander("ğŸ” Path Debug", expanded=False):
            st.text("\n".join(debug_info))
            st.text(f"CWD: {os.getcwd()}")
            st.text(f"__file__: {__file__}")
        
        # ìœ íš¨í•œ ê²½ë¡œ ì°¾ê¸°
        base_path = None
        for candidate in path_candidates:
            if candidate.exists():
                base_path = candidate
                break
        
        if base_path is None:
            st.sidebar.warning("No valid data path found!")
            return datasets
    else:
        base_path = Path(base_folder)
        if not base_path.exists():
            return datasets
    
    for folder in base_path.iterdir():
        if folder.is_dir():
            cache_folder = folder / "cache"
            metadata_path = cache_folder / "metadata.json"
            
            if metadata_path.exists():
                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)
                
                datasets.append({
                    'name': folder.name,
                    'path': str(folder),
                    'cache_path': str(cache_folder),
                    'created_at': metadata.get('created_at', 'Unknown'),
                    't31_records': metadata.get('t31_records', 0),
                    't41_records': metadata.get('t41_records', 0),
                    'flow_records': metadata.get('flow_records', 0),
                })
    
    return datasets

